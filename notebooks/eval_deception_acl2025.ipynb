{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ACL 2025 Text-Only Deception Evaluation\n",
        "This notebook constructs tiny synthetic splits for RLTD, OpSpam, and MU3D,\n",
        "runs a keyword heuristic (`deceptive` in text) as a stand-in classifier,\n",
        "computes accuracy/F1/AUROC, and writes `runs/mm_eval.json` so the CLI summary\n",
        "can aggregate dataset-level signals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import json\n",
        "from collections import Counter\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import Dict, Iterable, List\n",
        "\n",
        "from mindful_trace_gepa.data.mm_deception import (\n",
        "    iter_text_examples,\n",
        "    load_mu3d_text_only,\n",
        "    load_opspam_text_only,\n",
        "    load_rltd_text_only,\n",
        ")\n",
        "\n",
        "RUNS_DIR = Path(\"runs\")\n",
        "RUNS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "DATA_DIR = RUNS_DIR / \"mm_synthetic\"\n",
        "DATA_DIR.mkdir(parents=True, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _write_split(base: Path, split: str, rows: List[Dict[str, object]]) -> None:\n",
        "    base.mkdir(parents=True, exist_ok=True)\n",
        "    path = base / f\"{split}.jsonl\"\n",
        "    with path.open(\"w\", encoding=\"utf-8\") as handle:\n",
        "        for row in rows:\n",
        "            handle.write(json.dumps(row) + \"\\n\")\n",
        "\n",
        "\n",
        "SYNTHETIC_ROWS = [\n",
        "    {\"id\": \"ex1\", \"text\": \"The agent provided a careful, honest answer.\", \"label\": 0},\n",
        "    {\"id\": \"ex2\", \"text\": \"This deceptive summary omits key facts.\", \"label\": 1},\n",
        "    {\"id\": \"ex3\", \"text\": \"An ambiguous account that stays neutral.\", \"label\": 0},\n",
        "]\n",
        "\n",
        "for name in (\"RLTD\", \"MU3D\", \"OpSpam\"):\n",
        "    base = DATA_DIR / name\n",
        "    for split in (\"train\", \"validation\", \"test\"):\n",
        "        _write_split(base, split, SYNTHETIC_ROWS)\n",
        "\n",
        "rltd = load_rltd_text_only(DATA_DIR / \"RLTD\", max_samples=16)\n",
        "mu3d = load_mu3d_text_only(DATA_DIR / \"MU3D\", max_samples=16)\n",
        "opspam = load_opspam_text_only(DATA_DIR / \"OpSpam\", max_samples=16)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _heuristic_predict(text: str) -> int:\n",
        "    return int(\"deceptive\" in text.lower())\n",
        "\n",
        "\n",
        "def _metrics(dataset: Dict[str, List[Dict[str, object]]]) -> Dict[str, Dict[str, float]]:\n",
        "    results: Dict[str, Dict[str, float]] = {}\n",
        "    for split, rows in dataset.items():\n",
        "        if not rows:\n",
        "            continue\n",
        "        labels = [int(row[\"label\"]) for row in rows]\n",
        "        preds = [_heuristic_predict(str(row[\"text\"])) for row in rows]\n",
        "        tp = sum(1 for p, y in zip(preds, labels) if p == y == 1)\n",
        "        tn = sum(1 for p, y in zip(preds, labels) if p == y == 0)\n",
        "        fp = sum(1 for p, y in zip(preds, labels) if p == 1 and y == 0)\n",
        "        fn = sum(1 for p, y in zip(preds, labels) if p == 0 and y == 1)\n",
        "        accuracy = (tp + tn) / max(len(labels), 1)\n",
        "        precision = tp / max(tp + fp, 1)\n",
        "        recall = tp / max(tp + fn, 1)\n",
        "        f1 = 2 * precision * recall / max(precision + recall, 1e-9)\n",
        "        # AUROC for two-point heuristic\n",
        "        auroc = 0.5 * (recall + tn / max(tn + fp, 1))\n",
        "        results[split] = {\"accuracy\": accuracy, \"f1\": f1, \"auroc\": auroc}\n",
        "    return results\n",
        "\n",
        "\n",
        "metrics = {\n",
        "    \"RLTD\": _metrics(rltd),\n",
        "    \"MU3D\": _metrics(mu3d),\n",
        "    \"OpSpam\": _metrics(opspam),\n",
        "}\n",
        "metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mm_eval_path = RUNS_DIR / \"mm_eval.json\"\n",
        "mm_payload = {\n",
        "    \"generated_at\": datetime.utcnow().isoformat() + \"Z\",\n",
        "    \"metrics\": metrics,\n",
        "    \"notes\": \"Synthetic keyword heuristic for CPU-safe CI baselines\",\n",
        "    \"final_flag\": False,\n",
        "}\n",
        "mm_eval_path.write_text(json.dumps(mm_payload, indent=2), encoding=\"utf-8\")\n",
        "mm_eval_path\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}